import numpy as np
from scipy.integrate import solve_bvp
from scipy.interpolate import interp1d
from scipy.integrate import simpson
import matplotlib.pyplot as plt
from tqdm import tqdm

class Problem_Solver:

    def __init__(self, 
                 n = 1000, 
                 dt = 0.01, 
                 tol = 1e-4, 
                 alpha = 0.5, 
                 beta = 0.5, 
                 gamma = 0.15, 
                 tilde_sigma = 0.3, 
                 delta = 0.05, 
                 a_0 = 0.5, 
                 a_1 = 5, # a_I
                 a_2 = 2    , # a^I_m - a^S_m
                 a_3 = 5, # a_r
                 a_4 = 0.5, # a^S_m
                 x_min = 0.01, 
                 x_max = 0.99
                 ): 
        
        # Parameters
        self.n = n
        self.h = (x_max - x_min) / n  # Step size for the state space
        self.dt = dt
        self.tol_main = tol

        # Problem-specific parameters
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.tilde_sigma = tilde_sigma
        self.delta = delta

        # Coefficients for the reward function
        self.a_0 = a_0
        self.a_1 = a_1
        self.a_2 = a_2
        self.a_3 = a_3
        self.a_4 = a_4

        # Truncated boundaries of controlled SDE
        self.x_min = x_min
        self.x_max = x_max
        self.x_grid = np.linspace(self.x_min, self.x_max, self.n)

    def b_func(self, x, eta, rho):
        """
        Drift function b(x, eta, rho) = Œ∑ * Œ± * (1 - x) + x * (Œ∑^2 * Œ≤ * (1 - x) - (Œ≥ + œÅ)).
        """
        return eta * self.alpha * (1 - x) + x * (eta ** 2 * self.beta * (1 - x) - (self.gamma + rho))

    def sigma_func(self, x):
        """
        Diffusion function œÉ(x) = ùúè * x * (1 - x).
        """
        return self.tilde_sigma * x * (1 - x)

    def f_func(self, x, eta, rho):
        """
        Reward function f(x, Œ∑, œÅ) = a_0 + a_1 * x + a_2 * x * (1 - Œ∑)^2 + a_3 * x * œÅ^2.
        """
        return self.a_0 + self.a_1 * x + (self.a_2 * x + self.a_4) * (1 - eta)**2 + self.a_3 * x * rho**2

    def objective_func(
        self,
        x_initial,
        x_grid,
        eta,
        rho,
        num_simulations = 10000, 
        T_stopped = 75, 
        if_plot = False
    ):
        """
        Vectorized version:
        - X_t can go beyond [x_min, x_max].
        - Controls Œ∑ and œÅ are evaluated using boundary values when X_t is out of bounds.
        """

        T = int(T_stopped / self.dt) + 1
        t = np.linspace(0, T_stopped, T)

        X = np.zeros((num_simulations, T))
        X[:, 0] = x_initial

        # Controls will clip X internally when evaluated:
        def eta_func(x_eval):
            x_clipped = np.clip(x_eval, self.x_min, self.x_max)
            return np.interp(x_clipped, x_grid, eta)

        def rho_func(x_eval):
            x_clipped = np.clip(x_eval, self.x_min, self.x_max)
            return np.interp(x_clipped, x_grid, rho)

        dW = np.sqrt(self.dt) * np.random.randn(num_simulations, T - 1)

        for k in range(1, T):
            X_prev = X[:, k - 1]

            eta_t = eta_func(X_prev)  # control evaluated at clipped X
            rho_t = rho_func(X_prev)

            b = self.b_func(X_prev, eta_t, rho_t)
            sigma = self.sigma_func(X_prev)

            X[:, k] = X_prev + b * self.dt + sigma * dW[:, k - 1]

        f_X = self.f_func(X, eta_func(X), rho_func(X))
        discount = np.exp(-self.delta * t)
        discounted_f_X = f_X * discount

        integral = simpson(discounted_f_X, t, axis=1)

        if if_plot:
            plt.figure(figsize=(10, 6))
            for i in range(min(20, num_simulations)):
                plt.plot(t, X[i], alpha=0.6)
            plt.xlabel("Time (t)")
            plt.ylabel("State (X_t)")
            plt.title(f"Sample Paths (Up to 20 of {num_simulations})")
            plt.grid(True)
            plt.show()

        return t, X, integral

    
    def monte_carlo_value(
        self,
        x_initial,
        x_grid,
        eta,
        rho,
        num_simulations = 2000,
        if_plot = False
    ):
        """
        Compute mean value function from vectorized simulations.

        Returns:
        - estimated_value: scalar mean of all path integrals.
        """
        _, _, cumulative_rewards = self.objective_func(
            x_initial=x_initial,
            x_grid=x_grid,
            eta=eta,
            rho=rho,
            num_simulations=num_simulations,
            if_plot=if_plot
        )

        estimated_value = np.mean(cumulative_rewards)
        print(f"Estimated value function: {estimated_value:.4f}")

        return estimated_value

    def Bellman_solver(self, x_grid, v0, v1, eta, rho, if_plot = False):
        """
        Solve the Bellman equation using the shooting method with boundary value problem solver.

        Parameters:
        - v0: Boundary condition at x=0.1.
        - v1: Boundary condition at x=0.9.
        - eta: Control Œ∑.
        - rho: Control œÅ.
        - if_plot: Whether to plot the solution.

        Returns:
        - x_sol: The grid points where the solution is computed.
        - v_sol: The value function solution.
        - p_sol: The derivative of the value function (dv/dx).
        """
        # Interpolate eta and rho to handle any input shape
        eta_interp = interp1d(x_grid, eta, kind='cubic', fill_value='extrapolate')
        rho_interp = interp1d(x_grid, rho, kind='cubic', fill_value='extrapolate')

        def bellman_rhs(x, V):
            v, p = V
            eta_val = eta_interp(x)  # Interpolated Œ∑
            rho_val = rho_interp(x)  # Interpolated œÅ
            b_val = self.b_func(x, eta_val, rho_val)
            sigma_val = self.sigma_func(x)
            f_val = self.f_func(x, eta_val, rho_val)
            dv_dx = p
            dp_dx = (self.delta * v - b_val * p - f_val) / (0.5 * sigma_val**2)
            return np.array([dv_dx, dp_dx])

        def boundary_conditions(Va, Vb):
            return [Va[0] - v0, Vb[1] - v1]

        x = x_grid

        # Initial guess for the value function and its derivative
        v_guess = x **2  # Linear
        p_guess = 2 * x  # Constant derivative

        V_guess = np.vstack([v_guess, p_guess])

        solution = solve_bvp(bellman_rhs, boundary_conditions, x, V_guess, max_nodes=int(1e6), tol=1e-4)

        if solution.success:
            print("Solver converged successfully.")
        else:
            print("Solver failed to converge.")

        # x_sol = solution.x
        v_sol, p_sol = solution.sol(x)  # Value function

        if if_plot:
            plt.figure(figsize=(10, 8))
            plt.plot(x, v_sol, label="Value Function")
            # plt.plot(x, p_sol, label="Derivative of Value Function (dv/dx)", linestyle='--')
            plt.xlabel("State (x)")
            plt.ylabel("Value / Derivative")
            plt.title("Solution to Bellman ODE")
            plt.legend()
            plt.grid()
            plt.show()

        return x, v_sol, p_sol

    def L2_norm(self, x1, x2, y1, y2): 
        """
        Compute the L2 norm of the difference between two arrays of different length by interpolating.

        Parameters:
        - x1: First array of x values.
        - x2: Second array of x values.
        - y1: First array of y values.
        - y2: Second array of y values.
        Returns:
        - L2 norm of the difference.
        """
        if len(x1) == len(x2) and len(x1) == len(self.x_grid):
            # If lengths are the same, directly compute the L2 norm
            y1_interp = y1
            y2_interp = y2
        else:
            # Interpolate y1 to the length of y2
            y1_interp  = interp1d(x1, y1, kind = 'cubic', fill_value = 'extrapolate')(self.x_grid)
            y2_interp = interp1d(x2, y2, kind = 'cubic', fill_value = 'extrapolate')(self.x_grid)
        # Compute the normalized L2 norm
        return np.linalg.norm(y1_interp - y2_interp) / np.sqrt(len(self.x_grid))

    def policy_improve(self, eta_guess, rho_guess, if_plot = False, if_compare = True): 
        """
        Policy improvement step for the Bellman equation.

        Parameters:
        - eta_guess: Initial guess for control Œ∑.
        - rho_guess: Initial guess for control œÅ.
        - if_plot: Whether to plot the results.
        """
        # Initial guess for the value function
        v0 = self.monte_carlo_value(self.x_min, self.x_grid, eta_guess, rho_guess)
        v1_le = self.monte_carlo_value(self.x_max - 0.01, self.x_grid, eta_guess, rho_guess)
        v1_ri = self.monte_carlo_value(self.x_max, self.x_grid, eta_guess, rho_guess)
        p1 = (v1_ri - v1_le) / 0.01  # Numerical derivative at the right boundary

        x, v_int, p_int = self.Bellman_solver(self.x_grid, v0, p1, eta_guess, rho_guess, if_plot=False)
        if if_compare:
            x_begin = x.copy()
            v_begin = v_int.copy()
        iter = 0
        error = []

        while True:
            eta_new = np.clip(
                1 - ((self.alpha * (1 - x) + self.beta * x * (1 - x)) * p_int) / (2 * (self.a_2 * x + self.a_4)), 
                0, 1)
            rho_new = np.clip(p_int / (2 * self.a_3), 0, 100)
            # eta_new = eta_guess.copy()
            # rho_new = rho_guess.copy()

            v0_new = self.monte_carlo_value(self.x_min, x, eta_new, rho_new)
            v1_le_new = self.monte_carlo_value(self.x_max - 0.01, self.x_grid, eta_new, rho_new)
            v1_mo_new = self.monte_carlo_value(self.x_max, self.x_grid, eta_new, rho_new)
            p1_new = (v1_mo_new - v1_le_new) / 0.01  # Numerical derivative at the right boundary

            x, v_update, p_update = self.Bellman_solver(x, v0_new, p1_new, eta_new, rho_new, if_plot=False)

            if self.L2_norm(x, x, v_int, v_update) <= self.tol_main:
                print(f"Updated value function: {v_update}")
                print(f"Updated controls: Œ∑ = {eta_new}, œÅ = {rho_new}")
                break

            if iter >= 10: 
                print(f"Maximum iterations {iter} reached.")
                print(f"Updated value function: {v_update}")
                print(f"Updated controls: Œ∑ = {eta_new}, œÅ = {rho_new}")
                break

            error.append(self.L2_norm(x, x, v_int, v_update))
            iter += 1
            v_int = v_update
            p_int = p_update

            print(f"------------------The {iter}th iteration done: The normalized L2 norm = {error[-1]}-------------------")

        if if_plot:
            # Plot the error in a separate figure
            plt.figure(figsize=(10, 6))
            plt.plot(range(len(error)), error, label="Error", color='blue', marker='o')
            plt.xlabel("Iteration")
            plt.ylabel("Error")
            plt.title("Error Convergence", fontsize=14)
            plt.grid()
            plt.legend()
            plt.tight_layout()
            plt.show()

            # Plot the value function in a separate figure
            if if_compare:
                plt.figure(figsize=(10, 6))
                plt.plot(x_begin, v_begin, label="Initial Value Function", color='orange', linestyle='--')
                plt.plot(x, v_update, label="Final Value Function", color='purple')
                plt.xlabel("State (x)")
                plt.ylabel("Value (v)")
                plt.title("Comparison of Initial and Final Value Functions", fontsize=14)
                plt.legend()
                plt.tight_layout()
                plt.show()

            plt.figure(figsize=(10, 6))
            plt.plot(x, v_update, label="Value Function", color='purple')
            plt.xlabel("State (x)")
            plt.ylabel("Value (v)")
            plt.title("Solution to Bellman ODE", fontsize=14) 
            plt.legend()
            plt.tight_layout()
            plt.show()

            # Plot the controls Œ∑ and œÅ in another separate figure
            plt.figure(figsize=(10, 6))
            x_small = np.linspace(self.x_min + 0.01, self.x_max - 0.01, 500)
            eta_plotting = interp1d(x, eta_new, kind='cubic', fill_value='extrapolate')(x_small)
            rho_plotting = interp1d(x, rho_new, kind='cubic', fill_value='extrapolate')(x_small)
            plt.plot(x_small, eta_plotting, label="Control Œ∑", color='blue')
            plt.plot(x_small, rho_plotting, label="Control œÅ", color='red')
            plt.xlabel("State (x)")
            plt.ylabel("Control")
            plt.title("Controls Œ∑ and œÅ over State", fontsize=14)  
            plt.legend()
            plt.tight_layout()
            plt.show()

        # x_update = x.copy()

        return eta_new, rho_new, v_update
    
    def sensitivity_analysis(self, x, x_upd, eta, rho, v, baseline_eta = 0.5, baseline_rho = 1, if_plot=True):
        """
        Perform sensitivity analysis on the value function with respect to the controls Œ∑ and œÅ.

        Parameters:
        - eta: Control Œ∑.
        - rho: Control œÅ.
        - baseline: Baseline value for the controls (percentage perturbation).
        - if_plot: Whether to plot the results.

        Returns:
        - sensitivity_results: A dictionary containing the value functions for perturbed controls.
        """
        # Define perturbations for Œ∑ and œÅ
        perturbations = {
            'eta_up': np.clip(eta + baseline_eta, 0, 1),
            'eta_down': np.clip(eta - baseline_eta, 0, 1),
            'rho_up': np.maximum(0, rho + baseline_rho),
            'rho_down': np.maximum(0, rho - baseline_rho)
        }

        # Compute the value function for each perturbation
        sensitivity_results = {}
        for key, perturbed_control in perturbations.items():
            print(f'...............Starting sensitivity analysis for {key}..................')
            if 'eta' in key:
                # Perturb Œ∑ while keeping œÅ constant
                v0 = self.monte_carlo_value(self.x_min, x, perturbed_control, rho)
                v1 = self.monte_carlo_value(self.x_max, x, perturbed_control, rho)
                x_sensitive, value, _ = self.Bellman_solver(x, v0, v1, perturbed_control, rho)
            elif 'rho' in key:
                # Perturb œÅ while keeping Œ∑ constant
                v0 = self.monte_carlo_value(self.x_min, x, eta, perturbed_control)
                v1 = self.monte_carlo_value(self.x_max, x, eta, perturbed_control)
                x_sensitive, value, _ = self.Bellman_solver(x, v0, v1, eta, perturbed_control)
            sensitivity_results[key] = (x_sensitive, value)

        # Plot the sensitivity analysis results if requested
        if if_plot:
            plt.figure(figsize=(10, 6))

            # Fetch the x and y values for each perturbation from dictionary
            x_eta_up, y_eta_up = sensitivity_results['eta_up']
            x_eta_down, y_eta_down = sensitivity_results['eta_down']
            x_rho_up, y_rho_up = sensitivity_results['rho_up']
            x_rho_down, y_rho_down = sensitivity_results['rho_down']

            plt.plot(x_eta_up, y_eta_up, label="Œ∑ Up", color='blue', linestyle='--')
            plt.plot(x_eta_down, y_eta_down, label="Œ∑ Down", color='blue', linestyle=':')
            plt.plot(x_rho_up, y_rho_up, label="œÅ Up", color='red', linestyle='--')
            plt.plot(x_rho_down, y_rho_down, label="œÅ Down", color='red', linestyle=':')
            plt.plot(x_upd, v, label="Baseline", color='black')
            plt.xlabel("State (x)")
            plt.ylabel("Value Function")
            plt.title("Sensitivity Analysis of Value Function")
            plt.legend()
            plt.grid()
            plt.tight_layout()
            plt.show()

        return sensitivity_results

    def sensitivity_steps(self, x, eta, rho, v, if_plot=True): 
        """
        Perform stepwise sensitivity analysis on the value function with respect to the controls Œ∑ and œÅ.

        Parameters:
        - x: Original x grid.
        - eta: Control Œ∑.
        - rho: Control œÅ.
        - v: Baseline value function.
        - if_plot: Whether to plot the results.

        Returns:
        - sensitivity_results: A dictionary containing the value functions for each stepwise perturbation.
        """
        # Define stepwise perturbations for Œ∑ and œÅ
        eta_steps = [0.25, 0.5, 0.75]
        rho_steps = [0.5, 1, 2]

        sensitivity_results = {}

        # Stepwise perturbation for Œ∑ (plus and down)
        for step in eta_steps:
            perturbed_eta_plus = np.clip(eta + step, 0, 1)
            v0 = self.monte_carlo_value(self.x_min, x, perturbed_eta_plus, rho)
            v1 = self.monte_carlo_value(self.x_max, x, perturbed_eta_plus, rho)
            x_sensitive, value, _ = self.Bellman_solver(x, v0, v1, perturbed_eta_plus, rho)
            key = f"eta_plus_{step}"
            sensitivity_results[key] = (x_sensitive, value)

            perturbed_eta_down = np.clip(eta - step, 0, 1)
            v0 = self.monte_carlo_value(self.x_min, x, perturbed_eta_down, rho)
            v1 = self.monte_carlo_value(self.x_max, x, perturbed_eta_down, rho)
            x_sensitive, value, _ = self.Bellman_solver(x, v0, v1, perturbed_eta_down, rho)
            key = f"eta_down_{step}"
            sensitivity_results[key] = (x_sensitive, value)

        # Stepwise perturbation for œÅ (plus and down)
        for step in rho_steps:
            perturbed_rho_plus = np.maximum(0, rho + step)
            v0 = self.monte_carlo_value(self.x_min, x, eta, perturbed_rho_plus)
            v1 = self.monte_carlo_value(self.x_max, x, eta, perturbed_rho_plus)
            x_sensitive, value, _ = self.Bellman_solver(x, v0, v1, eta, perturbed_rho_plus)
            key = f"rho_plus_{step}"
            sensitivity_results[key] = (x_sensitive, value)

            perturbed_rho_down = np.maximum(0, rho - step)
            v0 = self.monte_carlo_value(self.x_min, x, eta, perturbed_rho_down)
            v1 = self.monte_carlo_value(self.x_max, x, eta, perturbed_rho_down)
            x_sensitive, value, _ = self.Bellman_solver(x, v0, v1, eta, perturbed_rho_down)
            key = f"rho_down_{step}"
            sensitivity_results[key] = (x_sensitive, value)

        # Plot eta_plus results in one figure
        if if_plot:
            # Œ∑ plus
            plt.figure(figsize=(10, 6))
            plt.plot(x, v, label="Baseline", color='black')
            for step in eta_steps:
                x_eta_plus, v_eta_plus = sensitivity_results[f"eta_plus_{step}"]
                plt.plot(x_eta_plus, v_eta_plus, label=f"Œ∑ + {step}", linestyle='--')
            plt.xlabel("State (x)")
            plt.ylabel("Value Function")
            plt.title("Stepwise Sensitivity: Œ∑ Increase")
            plt.legend()
            plt.grid()
            plt.tight_layout()
            plt.show()

            # Œ∑ down
            plt.figure(figsize=(10, 6))
            plt.plot(x, v, label="Baseline", color='black')
            for step in eta_steps:
                x_eta_down, v_eta_down = sensitivity_results[f"eta_down_{step}"]
                plt.plot(x_eta_down, v_eta_down, label=f"Œ∑ - {step}", linestyle=':')
            plt.xlabel("State (x)")
            plt.ylabel("Value Function")
            plt.title("Stepwise Sensitivity: Œ∑ Decrease")
            plt.legend()
            plt.grid()
            plt.tight_layout()
            plt.show()

            # œÅ plus
            plt.figure(figsize=(10, 6))
            plt.plot(x, v, label="Baseline", color='black')
            for step in rho_steps:
                x_rho_plus, v_rho_plus = sensitivity_results[f"rho_plus_{step}"]
                plt.plot(x_rho_plus, v_rho_plus, label=f"œÅ + {step}", linestyle='--')
            plt.xlabel("State (x)")
            plt.ylabel("Value Function")
            plt.title("Stepwise Sensitivity: œÅ Increase")
            plt.legend()
            plt.grid()
            plt.tight_layout()
            plt.show()

            # œÅ down
            plt.figure(figsize=(10, 6))
            plt.plot(x, v, label="Baseline", color='black')
            for step in rho_steps:
                x_rho_down, v_rho_down = sensitivity_results[f"rho_down_{step}"]
                plt.plot(x_rho_down, v_rho_down, label=f"œÅ - {step}", linestyle=':')
            plt.xlabel("State (x)")
            plt.ylabel("Value Function")
            plt.title("Stepwise Sensitivity: œÅ Decrease")
            plt.legend()
            plt.grid()
            plt.tight_layout()
            plt.show()

        return sensitivity_results

    def compare_a_groups(self, a_groups, eta_init=None, rho_init=None):
        """
        Compare value functions and controls for different (a_1, a_2, a_3) groups and plot them.

        Parameters:
        - a_groups: List of tuples [(a_1, a_2, a_3), ...]
        - eta_init: Optional initial eta array (default: zeros)
        - rho_init: Optional initial rho array (default: zeros)
        """
        if eta_init is None:
            eta_init = np.zeros(self.n)
        if rho_init is None:
            rho_init = np.zeros(self.n)

        legends = []
        eta_list = []
        rho_list = []
        x = np.linspace(self.x_min + 0.01, self.x_max - 0.01, 500)

        for values in a_groups:
            # Set coefficients
            self.a_4 = values
            self.a_2 = 2.5 - values 

            # Solve for optimal policy and value function
            eta_opt, rho_opt, _ = self.policy_improve(eta_init, rho_init, if_plot=False, if_compare=False)
            eta_interp = interp1d(self.x_grid, eta_opt, kind='cubic', fill_value='extrapolate')(x)
            rho_interp = interp1d(self.x_grid, rho_opt, kind='cubic', fill_value='extrapolate')(x)
            eta_list.append(eta_interp)
            rho_list.append(rho_interp)
            legends.append(f"a_m^S={values}")

        fig, axs = plt.subplots(2, 1, figsize=(10, 15), sharex=True)

        # Plot eta controls
        for eta, label in zip(eta_list, legends):
            axs[0].plot(x, eta, lw=2, label=label)
        axs[0].set_ylabel("Control Œ∑")
        axs[0].set_title("Optimal Œ∑ for Different Groups")
        axs[0].legend()

        # Plot rho controls
        for rho, label in zip(rho_list, legends):
            axs[1].plot(x, rho, lw=2, label=label)
        axs[1].set_xlabel("State (x)")
        axs[1].set_ylabel("Control œÅ")
        axs[1].set_title("Optimal œÅ for Different Groups")
        axs[1].legend()

        plt.tight_layout()
        plt.show()

def main():
    solver = Problem_Solver()
    eta = 1 * np.ones(solver.n)
    rho = 1 * np.ones(solver.n)
    # eta, rho, v = solver.policy_improve(eta, rho, if_plot=True, if_compare=False)

    # # Comparison of MC and Bellman value function at 10 points
    # x_points = np.linspace(solver.x_min, solver.x_max, 10)
    # v_mc = []

    # for x0 in x_points:
    #     val_mc = solver.monte_carlo_value(
    #         x_initial=x0,
    #         x_grid=solver.x_grid,
    #         eta=eta,
    #         rho=rho,
    #         num_simulations=2000,
    #         if_plot=False
    #     )
    #     v_mc.append(val_mc)

    # # Interpolate Bellman value function to x_points for printing
    # v_bellman = np.interp(x_points, solver.x_grid, v)

    # # Print comparison
    # print("x\tMC Value\tBellman Value")
    # for xi, vmc, vbell in zip(x_points, v_mc, v_bellman):
    #     print(f"{xi:.3f}\t{vmc:.4f}\t\t{vbell:.4f}")

    # # Plot: MC points only, Bellman value function as curve
    # import matplotlib.pyplot as plt
    # plt.figure(figsize=(10, 6))
    # plt.plot(solver.x_grid, v, '-', label='Bellman Value Function')  # continuous curve
    # plt.scatter(x_points, v_mc, color='red', label='Monte Carlo Value (points)', zorder=5)  # points only
    # plt.xlabel("State (x)")
    # plt.ylabel("Value Function")
    # plt.title("Monte Carlo Points vs Bellman Value Function")
    # plt.legend()
    # plt.grid()
    # plt.tight_layout()
    # plt.show()
    solver.monte_carlo_value(
            x_initial=0.99,
            x_grid=solver.x_grid,
            eta=eta,
            rho=rho,
            num_simulations=2000,
            if_plot=False
    )

# Run the main function if the script is executed directly
if __name__ == "__main__":
    main()