import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import torch
import Envir  # ç¯å¢ƒæ–‡ä»¶

def plot_optimal_policy_and_q(agent, env, dp_solver=None):
    """ç»˜åˆ¶æœ€ä¼˜ç­–ç•¥å’ŒQå‡½æ•°"""
    
    # åˆ›å»ºç½‘æ ¼
    x_states = np.arange(env.h, 1.0, env.h)
    regimes = env.regimes
    
    # åˆå§‹åŒ–å­˜å‚¨æ•°ç»„
    Q_values = np.zeros((len(x_states), len(regimes), env.n_actions))
    policy = np.zeros((len(x_states), len(regimes)))
    policy_actions = np.zeros((len(x_states), len(regimes), 2))  # å­˜å‚¨å…·ä½“åŠ¨ä½œ
    
    # è®¡ç®—Qå€¼å’Œç­–ç•¥
    agent.policy_net.eval()
    with torch.no_grad():
        for i, x in enumerate(x_states):
            for j, l in enumerate(regimes):
                state = torch.tensor([x, l], dtype=torch.float32).to(agent.device)
                q_vals = agent.policy_net(state).cpu().numpy()
                Q_values[i, j] = q_vals
                best_action_idx = np.argmin(q_vals)  # æœ€å°åŒ–æˆæœ¬
                policy[i, j] = best_action_idx
                
                # è·å–å…·ä½“åŠ¨ä½œå€¼
                action = env.actions[int(best_action_idx)]
                if isinstance(action, (list, tuple, np.ndarray)) and len(action) == 2:
                    policy_actions[i, j, 0] = action[0]  # eta
                    policy_actions[i, j, 1] = action[1]  # rho
                else:
                    policy_actions[i, j, 0] = action
                    policy_actions[i, j, 1] = action

    # åˆ›å»ºå›¾å½¢
    fig = plt.figure(figsize=(20, 12))
    
    # 1. æœ€ä¼˜Qå€¼æ›²é¢ (3D)
    ax1 = fig.add_subplot(2, 3, 1, projection='3d')
    X, L = np.meshgrid(x_states, regimes, indexing='ij')
    optimal_Q = np.min(Q_values, axis=2)  # æ¯ä¸ªçŠ¶æ€çš„æœ€å°Qå€¼
    
    surf = ax1.plot_surface(X, L, optimal_Q, cmap='viridis', alpha=0.8)
    ax1.set_xlabel('Infection Rate (x)')
    ax1.set_ylabel('Regime (l)')
    ax1.set_zlabel('Optimal Q-Value')
    ax1.set_title('Optimal Q-Function Surface')
    fig.colorbar(surf, ax=ax1, shrink=0.5, aspect=20)
    
    # 2. æœ€ä¼˜Qå€¼ç­‰é«˜çº¿
    ax2 = fig.add_subplot(2, 3, 2)
    contour = ax2.contourf(X, L, optimal_Q, levels=20, cmap='viridis')
    ax2.set_xlabel('Infection Rate (x)')
    ax2.set_ylabel('Regime (l)')
    ax2.set_title('Optimal Q-Function Contour')
    plt.colorbar(contour, ax=ax2)
    
    # 3. ç­–ç•¥çƒ­å›¾
    ax3 = fig.add_subplot(2, 3, 3)
    im = ax3.imshow(policy.T, extent=[env.h, 1.0-env.h, regimes[-1], regimes[0]], 
                   aspect='auto', cmap='tab10')
    ax3.set_xlabel('Infection Rate (x)')
    ax3.set_ylabel('Regime (l)')
    ax3.set_title('Optimal Policy (Action Index)')
    plt.colorbar(im, ax=ax3)
    
    # 4. etaç­–ç•¥åˆ†é‡
    ax4 = fig.add_subplot(2, 3, 4)
    for j, l in enumerate(regimes):
        ax4.plot(x_states, policy_actions[:, j, 0], label=f'Regime {l}', linewidth=2)
    ax4.set_xlabel('Infection Rate (x)')
    ax4.set_ylabel('Î· Policy')
    ax4.set_title('Î· Component of Optimal Policy')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # 5. rhoç­–ç•¥åˆ†é‡
    ax5 = fig.add_subplot(2, 3, 5)
    for j, l in enumerate(regimes):
        ax5.plot(x_states, policy_actions[:, j, 1], label=f'Regime {l}', linewidth=2)
    ax5.set_xlabel('Infection Rate (x)')
    ax5.set_ylabel('Ï Policy')
    ax5.set_title('Ï Component of Optimal Policy')
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    
    # 6. ä¸DPè§£å¯¹æ¯” (å¦‚æœæœ‰çš„è¯)
    ax6 = fig.add_subplot(2, 3, 6)
    if dp_solver is not None:
        # è®¡ç®—DPè§£
        dp_q_values = np.zeros((len(x_states), len(regimes)))
        for i, x in enumerate(x_states):
            for j, l in enumerate(regimes):
                dp_q_values[i, j] = dp_solver.get_optimal_value(x, l)
        
        # ç»˜åˆ¶å¯¹æ¯”
        line1, = ax6.plot(x_states, optimal_Q[:, 0], 'b-', label='DQN Regime 0', linewidth=2)
        line2, = ax6.plot(x_states, optimal_Q[:, 1], 'r-', label='DQN Regime 1', linewidth=2)
        line3, = ax6.plot(x_states, dp_q_values[:, 0], 'b--', label='DP Regime 0', linewidth=2, alpha=0.7)
        line4, = ax6.plot(x_states, dp_q_values[:, 1], 'r--', label='DP Regime 1', linewidth=2, alpha=0.7)
        
        ax6.set_xlabel('Infection Rate (x)')
        ax6.set_ylabel('Optimal Value')
        ax6.set_title('DQN vs DP Solution Comparison')
        ax6.legend()
        ax6.grid(True, alpha=0.3)
    else:
        # å¦‚æœæ²¡æœ‰DPè§£ï¼Œæ˜¾ç¤ºQå€¼éšçŠ¶æ€çš„å˜åŒ–
        for j, l in enumerate(regimes):
            ax6.plot(x_states, optimal_Q[:, j], label=f'Regime {l}', linewidth=2)
        ax6.set_xlabel('Infection Rate (x)')
        ax6.set_ylabel('Optimal Q-Value')
        ax6.set_title('Optimal Q-Value by Regime')
        ax6.legend()
        ax6.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # æ‰“å°ç­–ç•¥ç»Ÿè®¡
    print("\nğŸ“Š ç­–ç•¥åˆ†æ:")
    print(f"åŠ¨ä½œç©ºé—´å¤§å°: {env.n_actions}")
    print(f"çŠ¶æ€ç©ºé—´å¤§å°: {len(x_states)} x {len(regimes)}")
    
    # åˆ†æç­–ç•¥æ¨¡å¼
    unique_actions = np.unique(policy)
    print(f"ä½¿ç”¨çš„ç‹¬ç‰¹åŠ¨ä½œæ•°é‡: {len(unique_actions)}")
    
    for action_idx in unique_actions:
        count = np.sum(policy == action_idx)
        percentage = count / policy.size * 100
        action_desc = env.actions[int(action_idx)] if hasattr(env, 'actions') else f"Action {action_idx}"
        print(f"  åŠ¨ä½œ {action_idx} ({action_desc}): {count} çŠ¶æ€ ({percentage:.1f}%)")

def plot_q_value_distribution(agent, env):
    """ç»˜åˆ¶Qå€¼åˆ†å¸ƒç»Ÿè®¡"""
    x_states = np.arange(env.h, 1.0, env.h)
    regimes = env.regimes
    
    all_q_values = []
    agent.policy_net.eval()
    
    with torch.no_grad():
        for x in x_states:
            for l in regimes:
                state = torch.tensor([x, l], dtype=torch.float32).to(agent.device)
                q_vals = agent.policy_net(state).cpu().numpy()
                all_q_values.extend(q_vals)
    
    all_q_values = np.array(all_q_values)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # Qå€¼åˆ†å¸ƒç›´æ–¹å›¾
    ax1.hist(all_q_values, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    ax1.axvline(np.mean(all_q_values), color='red', linestyle='--', 
                label=f'Mean: {np.mean(all_q_values):.3f}')
    ax1.axvline(np.median(all_q_values), color='green', linestyle='--', 
                label=f'Median: {np.median(all_q_values):.3f}')
    ax1.set_xlabel('Q-Value')
    ax1.set_ylabel('Frequency')
    ax1.set_title('Q-Value Distribution')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # Qå€¼èŒƒå›´ç»Ÿè®¡
    q_stats = {
        'Min': np.min(all_q_values),
        'Max': np.max(all_q_values), 
        'Mean': np.mean(all_q_values),
        'Std': np.std(all_q_values),
        '25%': np.percentile(all_q_values, 25),
        '75%': np.percentile(all_q_values, 75)
    }
    
    ax2.bar(range(len(q_stats)), list(q_stats.values()), color='lightcoral')
    ax2.set_xticks(range(len(q_stats)))
    ax2.set_xticklabels(list(q_stats.keys()))
    ax2.set_ylabel('Q-Value')
    ax2.set_title('Q-Value Statistics')
    
    # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
    for i, v in enumerate(q_stats.values()):
        ax2.text(i, v + 0.1 * max(q_stats.values()), f'{v:.2f}', 
                ha='center', va='bottom')
    
    plt.tight_layout()
    plt.show()
    
    print(f"\nğŸ“ˆ Qå€¼ç»Ÿè®¡:")
    for stat, value in q_stats.items():
        print(f"  {stat}: {value:.4f}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    env = Envir.PandemicControlEnvironment()
    
    # å‡è®¾ä½ å·²ç»è®­ç»ƒå¥½äº†agent
    # agent, evaluation_results = train_dqn(env, episodes=10000, max_steps=100)
    
    # ç»˜åˆ¶å›¾åƒ
    # plot_optimal_policy_and_q(agent, env)
    # plot_q_value_distribution(agent, env)
    
    print("å›¾åƒç»˜åˆ¶å‡½æ•°å·²å®šä¹‰ï¼Œåœ¨è®­ç»ƒå®Œæˆåè°ƒç”¨å³å¯")